{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0cc1479-bb17-4f29-94e0-e11422103804",
   "metadata": {},
   "source": [
    "# Sentiment Analysis using cardiffnlp/twitter-roberta-base-sentiment-latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b8ae7df-c140-4e28-9e96-ab0b8975eb61",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'negative', 'score': 0.7235770225524902}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "\n",
    "\n",
    "MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "sentiment_task = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "sentiment_task(\"Covid cases are increasing fast!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f862980-cb2f-40c9-8b60-3603d4698877",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'neutral', 'score': 0.7895824909210205}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_task(\"SocialMediaFeed9@DigitalDaily: Nvidia's stock feels the heat as expiry of essential patents looms. Will innovation cool the stock? #IPexpiry #InvestorAlert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bce6c0b7-eb22-494f-813e-513de61c16f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c15d288-222b-4d1f-ab53-a010520bb312",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"training.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de3e4c69-49f4-4f86-a24e-3a864a831ab1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SocialMediaFeed</th>\n",
       "      <th>NVDA</th>\n",
       "      <th>ING</th>\n",
       "      <th>SAN</th>\n",
       "      <th>PFE</th>\n",
       "      <th>CSCO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@PharmaNews: Pfizer faces backlash over possib...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.029512</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@BusinessReport: A recent study found that mos...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@HardwareHubs: NVIDIA's contributions to a maj...</td>\n",
       "      <td>0.026125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@HealthWatch: Johnson &amp; Johnson faces lawsuits...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@IndustryInsider: Magnificent Honary faces pro...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     SocialMediaFeed      NVDA  ING  SAN  \\\n",
       "0  @PharmaNews: Pfizer faces backlash over possib...  0.000000  0.0  0.0   \n",
       "1  @BusinessReport: A recent study found that mos...  0.000000  0.0  0.0   \n",
       "2  @HardwareHubs: NVIDIA's contributions to a maj...  0.026125  0.0  0.0   \n",
       "3  @HealthWatch: Johnson & Johnson faces lawsuits...  0.000000  0.0  0.0   \n",
       "4  @IndustryInsider: Magnificent Honary faces pro...  0.000000  0.0  0.0   \n",
       "\n",
       "        PFE  CSCO  \n",
       "0 -0.029512   0.0  \n",
       "1  0.000000   0.0  \n",
       "2  0.000000   0.0  \n",
       "3  0.000000   0.0  \n",
       "4  0.000000   0.0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ce4f27b6-6880-4fee-a5bf-4c8bdfdd4cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments = [sentiment_task(x) for x in data[\"SocialMediaFeed\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3423e7cf-de94-42e3-b86f-078f530a5e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@PharmaNews: Pfizer faces backlash over possible closure of regional office. #PharmaNews #RegionalOffice [{'label': 'negative', 'score': 0.5853261351585388}]\n",
      "@BusinessReport: A recent study found that most CEOs only read business books. That explains a lot. #CEOReads #BusinessBooks [{'label': 'neutral', 'score': 0.6953707933425903}]\n",
      "@HardwareHubs: NVIDIA's contributions to a major industry collaboration have given the stock a boost. #IndustryCollaboration #GraphicsChip [{'label': 'positive', 'score': 0.9469926953315735}]\n",
      "@HealthWatch: Johnson & Johnson faces lawsuits over product safety concerns. #Lawsuits #ProductSafety [{'label': 'negative', 'score': 0.5267253518104553}]\n",
      "@IndustryInsider: Magnificent Honary faces production delays. #ProductionDelays #IndustryNews [{'label': 'negative', 'score': 0.7787243127822876}]\n",
      "@SocialMediaRumor: Unverified sources hint at Facebook's possible data breach. #DataBreach [{'label': 'negative', 'score': 0.625958263874054}]\n",
      "@USFastFoodNews: McDonald's facing heat over pesticide contamination reports. #PesticideContamination #FastFoodNews [{'label': 'negative', 'score': 0.7150940895080566}]\n",
      "@TechTrends: Cisco faces challenges in its supply chain, affecting the stock. A tough challenge to overcome. #SupplyChainIssues #TechStocks [{'label': 'negative', 'score': 0.5373494625091553}]\n",
      "@DigitalDaily: Nvidia's stock feels the heat as expiry of essential patents looms. Will innovation cool the stock? #IPexpiry #InvestorAlert [{'label': 'neutral', 'score': 0.7673906683921814}]\n",
      "@PharmaFlash: Pfizer faces minor product recall due to a minor design flaw. A minor hiccup for the company? #ProductRecall #PharmaNews [{'label': 'negative', 'score': 0.6434832215309143}]\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(data[\"SocialMediaFeed\"][i], sentiments[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d92690-923a-4c3d-9845-c3ea82e85349",
   "metadata": {},
   "source": [
    "# Classification of Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9af23073-434b-4f51-96b5-9ad6b9286cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "71556948-33cb-47c8-963b-bbaa2e8b64f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derive the 'label' column\n",
    "def get_label(row):\n",
    "    for col in ['NVDA', 'ING', 'SAN', 'PFE', 'CSCO']:\n",
    "        if row[col] != 0.0:\n",
    "            return col\n",
    "    return 'None'\n",
    "\n",
    "df['label'] = df.apply(get_label, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9760c8bb-769a-42c7-810a-c4c9f0d5f371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "df['encoded_label'] = label_encoder.fit_transform(df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4c3d89fc-a7ed-461f-8a26-e093eee02bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/ec2-user/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/ec2-user/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/ec2-user/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert the DataFrame to a Dataset\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Initialize the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "805625cc-ba5b-412c-a3b9-ece4dc0ea30d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce2550ba00ba4b31926884c606fe4bc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['SocialMediaFeed'], padding='max_length', truncation=True, max_length=128)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "88e93ba3-7c47-4e40-b2a0-528814b83e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tokenized dataset to pandas DataFrame\n",
    "tokenized_df = tokenized_datasets.to_pandas()\n",
    "\n",
    "# Keep only necessary columns\n",
    "tokenized_df = tokenized_df[['input_ids', 'attention_mask', 'encoded_label']]\n",
    "\n",
    "# Rename 'encoded_label' to 'labels' so that the Trainer recognizes it as the label column\n",
    "tokenized_df = tokenized_df.rename(columns={'encoded_label': 'labels'})\n",
    "\n",
    "# Split the DataFrame into train and test\n",
    "train_df, test_df = train_test_split(tokenized_df, test_size=0.2)\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c652355c-9b91-4a40-951b-ab10e6deadda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/ec2-user/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/ec2-user/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the BERT model for sequence classification\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "60e249e5-153d-491d-aa16-9f3e5058deb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "# Define training arguments and initialize Trainer\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    output_dir='./results',\n",
    "    overwrite_output_dir=True,\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "3ebc674a-750a-4b91-8b1f-1fe18c0b9aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 945\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 357\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='357' max='357' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [357/357 21:54, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.389800</td>\n",
       "      <td>0.643219</td>\n",
       "      <td>0.772152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.504300</td>\n",
       "      <td>0.438393</td>\n",
       "      <td>0.839662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.195200</td>\n",
       "      <td>0.458809</td>\n",
       "      <td>0.852321</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 237\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/checkpoint-119\n",
      "Configuration saved in ./results/checkpoint-119/config.json\n",
      "Model weights saved in ./results/checkpoint-119/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 237\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/checkpoint-238\n",
      "Configuration saved in ./results/checkpoint-238/config.json\n",
      "Model weights saved in ./results/checkpoint-238/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 237\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/checkpoint-357\n",
      "Configuration saved in ./results/checkpoint-357/config.json\n",
      "Model weights saved in ./results/checkpoint-357/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=357, training_loss=0.5400188894164997, metrics={'train_runtime': 1319.0449, 'train_samples_per_second': 2.149, 'train_steps_per_second': 0.271, 'total_flos': 186486657799680.0, 'train_loss': 0.5400188894164997, 'epoch': 3.0})"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=lambda p: {'accuracy': (p.predictions.argmax(-1) == p.label_ids).mean()}  # Optional: compute accuracy\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "05e675f9-f875-49c6-835d-7200f7d62e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 237\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 00:23]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "results = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e8c4f166-7160-4a7a-a4c9-1a0702c48fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4588092565536499, 'eval_accuracy': 0.8523206751054853, 'eval_runtime': 24.9199, 'eval_samples_per_second': 9.51, 'eval_steps_per_second': 1.204, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "1cb7f95d-6217-4978-af06-d06abde4990b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tweet is about: None\n"
     ]
    }
   ],
   "source": [
    "def predict(tweet, model, tokenizer, label_encoder):\n",
    "    # Tokenize the tweet\n",
    "    inputs = tokenizer(tweet, return_tensors=\"pt\", truncation=True, max_length=128, padding=\"max_length\")\n",
    "\n",
    "    # Get model's prediction\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "\n",
    "    # Get the predicted label ID\n",
    "    predicted_label_id = torch.argmax(logits, dim=1).item()\n",
    "\n",
    "    # Decode the label ID to get the company name\n",
    "    predicted_label = label_encoder.inverse_transform([predicted_label_id])[0]\n",
    "\n",
    "    return predicted_label\n",
    "\n",
    "# Example usage\n",
    "tweet = \"SocialMediaFeed15@TechBuzz: Unverified reports suggest can achive their sustainibility goals \"\n",
    "predicted_company = predict(tweet, model, tokenizer, label_encoder)\n",
    "print(f\"The tweet is about: {predicted_company}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49cc7a6-2cef-4488-868a-948cd0bf8afe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
